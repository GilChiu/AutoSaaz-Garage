# robots.txt for Localhost or Open Access Website
# Allow everything for all bots

User-agent: *
Disallow:

# Specific crawlers - explicitly allowed
User-agent: Googlebot
Disallow:

User-agent: Bingbot
Disallow:

User-agent: Slurp  # Yahoo
Disallow:

User-agent: DuckDuckBot
Disallow:

User-agent: Baiduspider
Disallow:

User-agent: Yandex
Disallow:

User-agent: Sogou
Disallow:

User-agent: Exabot
Disallow:

User-agent: facebot  # Facebook
Disallow:

User-agent: facebookexternalhit
Disallow:

User-agent: ia_archiver  # Alexa
Disallow:

User-agent: Applebot
Disallow:

User-agent: AhrefsBot
Disallow:

User-agent: SemrushBot
Disallow:

User-agent: MJ12bot
Disallow:

User-agent: DotBot
Disallow:

User-agent: PetalBot  # Huawei
Disallow:

User-agent: Twitterbot
Disallow:

User-agent: GPTBot  # OpenAI crawler
Disallow:

User-agent: ClaudeBot  # Anthropic's crawler
Disallow:

# Crawl-delay (optional, not set here)
# Crawl-delay: 10

# Sitemap (optional, for public deployment)
# Replace with your actual sitemap URL when deployed
# Sitemap: https://yourdomain.com/sitemap.xml
